from langchain_openai import ChatOpenAI
# from langchain_community.chat_models.openai import ChatOpenAI
import os
from dotenv import load_dotenv, find_dotenv
from typing import Callable, Awaitable

# åŠ è½½ç¯å¢ƒå˜é‡
# 1. å…ˆå°è¯•ä»å½“å‰ç›®å½•åŠ è½½.envæ–‡ä»¶
# 2. å¦‚æœæ‰¾ä¸åˆ°,åˆ™å°è¯•ä»çˆ¶ç›®å½•é€’å½’æŸ¥æ‰¾
print("åŠ è½½ç¯å¢ƒå˜é‡...")
dotenv_path = find_dotenv(raise_error_if_not_found=True)
load_dotenv(dotenv_path, override=True)

# è·å–å¿…éœ€çš„ç¯å¢ƒå˜é‡å¹¶è¿›è¡ŒéªŒè¯
print(os.getenv("OPENAI_LLM_BASE_URL"))
print(os.getenv("OPENAI_LLM_MODEL"))
print(os.getenv("LOCAL_LLM_BASE_URL"))
print(os.getenv("LOCAL_LLM_MODEL"))

DEFAULT_MODEL = "gpt-4o-mini"


# åˆ›å»ºæœ¬åœ° LLM å®ä¾‹
local_llm = ChatOpenAI(
    api_key="EMPTY",     # type: ignore
    base_url=os.getenv("LOCAL_LLM_BASE_URL"),
    model=os.getenv("LOCAL_LLM_MODEL") or DEFAULT_MODEL
)
openai_llm = ChatOpenAI(
    api_key=os.getenv("OPENAI_LLM_API_KEY"), # type: ignore
    base_url=os.getenv("OPENAI_LLM_BASE_URL"),
    model=os.getenv("OPENAI_LLM_MODEL"), # type: ignore
    temperature=0.3,
)


# åˆ›å»º OpenAI LLM å®ä¾‹
# æ³¨æ„: ç”±äº OpenAI çš„é™åˆ¶,éœ€è¦æ˜¾å¼è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡
# os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_LLM_API_KEY")
# import openai

# openai.base_url = os.getenv("OPENAI_LLM_BASE_URL")
# openai.api_key = os.getenv("OPENAI_LLM_API_KEY")

# openai_llm_naive = openai.Client(
#     base_url=os.getenv("OPENAI_LLM_BASE_URL"),
#     api_key=os.getenv("OPENAI_LLM_API_KEY"),
#     timeout=30.0
# )

# import openai
# def create__llm(base_url: str, api_key: str, model: str, stop: list[str] | None = None)

def create_llm_invoker(llm: ChatOpenAI, stop: list[str] | None = None) -> Callable[[str], Awaitable[str]]:
    async def ainvoke(prompt: str) -> str:
        if stop is None:
            return (await llm.ainvoke(prompt)).content # type: ignore
        else:
            content = ""
            # ğŸ”„ ä¿®å¤: ç§»é™¤å¤šä½™çš„ awaitï¼Œå› ä¸º astream() å·²ç»è¿”å›äº†ä¸€ä¸ª AsyncIterator
            async for chunk in llm.astream(prompt):
                # ğŸ›‘ æ£€æŸ¥æ˜¯å¦åŒ…å«åœæ­¢è¯
                for stop_token in stop:
                    if stop_token in chunk.content:
                        return content
                content += chunk.content # type: ignore
            return content
    return ainvoke

def chat_completion(prompt: str, model: str = os.getenv("OPENAI_LLM_MODEL") or DEFAULT_MODEL) -> str:
    """è°ƒç”¨ OpenAI API è¿›è¡Œå¯¹è¯è¡¥å…¨

    å‚æ•°:
        prompt: è¾“å…¥æç¤ºè¯
        model: æ¨¡å‹åç§°,é»˜è®¤ä»ç¯å¢ƒå˜é‡è·å–

    è¿”å›:
        str: æ¨¡å‹çš„å›å¤å†…å®¹
    """
    try:
        import requests

        headers = {
            "Authorization": f"Bearer {os.getenv('OPENAI_LLM_API_KEY')}",
            "Content-Type": "application/json"
        }

        data = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}]
        }

        response = requests.post(
            f"{os.getenv('OPENAI_LLM_BASE_URL')}/chat/completions",
            headers=headers,
            json=data,
            timeout=30.0
        )

        if response.status_code == 200:
            print(response.json())
            return response.json()["choices"][0]["message"]["content"]
        else:
            print(f"API è¯·æ±‚å¤±è´¥: {response.status_code}")
            return ""

    except Exception as e:
        print(f"OpenAI API è°ƒç”¨å‡ºé”™: {str(e)}")
        return ""



if __name__ == "__main__":
    # print("invoke openai_llm")
    # print(openai_llm.openai_api_base, openai_llm.openai_api_key, openai_llm.model_name)
    # print(chat_completion("Hello, world!"))
    # æµ‹è¯•å¼‚æ­¥è°ƒç”¨
    # import asyncio
    # from langchain_openai import ChatOpenAI

    # async def test_ainvoke():
    #     """ğŸ§ª æµ‹è¯•å¼‚æ­¥è°ƒç”¨"""
    #     response = await ainvoke("ä½ å¥½,è¯·ç”¨ä¸­æ–‡å›ç­”:1+1ç­‰äºå‡ ?è¯·ä½ ç”¨ä¸¤è¡Œå›ç­”", llm=openai_llm, stop=["\n"])
    #     print(f"ğŸ¤– å¼‚æ­¥è°ƒç”¨ç»“æœ: {response}")

    # # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    # asyncio.run(test_ainvoke())



    pass
